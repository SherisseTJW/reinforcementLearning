# Learning from Exploration

Suppose learning updates occured after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time ( but not the tendency to explore ), then the state values would converge to a different set of probabilities. What ( conceptually ) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? 

Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?

---